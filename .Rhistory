bhat0 <- mean(y) + bhat1*mean(x)
fit <- lm(y ~ x)$coeff
rm(bhat1, fit)
rm(bhat0, fit)
bhat1 <- cov(y, x)/var(x)
bhat0 <- mean(y) - bhat1*mean(x)
yhat <- bhat0 + bhat1*x
e <- y - yat
e <- y - yhat
s2 <- sum(e^2)/(length(e)-2)
q <- bhat1/s2
pt(q, length(e)-2)
q <- bhat0/s2
pt(q, length(e)-2)
1 - pt(q, length(e)-2)
(1 - pt(q, length(e)-2))*2
rm(q)
s2b1 <- s2/sum((x - mean(x))^2)
q <- bhat1/s2b1
pt(q, length(e) - 2)
pt(q, length(e) - 2, lower.tail = FALSE)
rm(q, s2b1)
s2b1 <- s2/sum((x - mean(x))^2)
q <- bhat1/sqrt(s2b1)
pt(q, lenght(x) - 2, lower.tail = FALSE)
pt(q, length(x) - 2, lower.tail = FALSE)
pt(q, length(x) - 2)
pt(q, length(x) - 2, lower.tail = FALSE)*2
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
bhat1 <- cov(y, x)/var(x)
bhat0 <- mean(y) - bhat1*mean(x)
yhat <- bhat0 + bhat1*x
e <- y - yhat
s2 <- sum(e^2)/(length(e)-2)
s <- sqrt(s2)
library(UsingR)
load(mtcars)
mtcars
data <- mtcars
x <- data$wt
y <- data$mpg
mx <- mean(x)
my <- mean(y)
alpha <- 0.05
bhat1 <- cov(y, x)/var(x)
bhat0 <- my - bhat1*mx
yhat <- bhat0 + bhat1*x
e <- y - yhat
n <- length(x)
s2 <- sum(e^2)/(n-2)
s <- sqrt(s2)
my +c(-1, 1)*qt(1-alpha/2, n-2, lower.tail = FALSE)*s
my +c(-1, 1)*qt(1-alpha/2, n-2, lower.tail = FALSE)*s/sqrt(n)
?cars
?mtcars
mtcars
y <- mtcars$mpg
x <- mtcars$wt
bhat1 <- cov(y, x)/var(x)
bhat0 <- mean(y) - bhat1*mean(x)
Yi <- bhat0 + bhat1*3
yhat <- bhat0 + bhat1*x
e <- y - yhat
n <- length(x)
s2 <- sum(e^2)/(n-2)
s <- sqrt(s2)
alpha <- 0.05
Yi + c(-1, 1)*qt(1-(alpha/2), n-2, lower.tail = FALSE)*s/sqrt(n)
Yi + c(-1, 1)*qt(1-(alpha/2), n-2, lower.tail = FALSE)*s
Yi + c(-1, 1)*qt(1-(alpha/2), 1, lower.tail = FALSE)*s/sqrt(n)
Yi + c(-1, 1)*qt(1-(alpha/2), n-2, lower.tail = FALSE)*s
bhat1 <- cov(y, x)/var(x)
bhat0 <- mean(y) - bhat1*mean(x)
e <- y - yhat
s2 <- sum(e^2)/(n-2)
s <- sqrt(n)
s <- sqrt(s2)
Yi + c(-1, 1)*qt(1-(alpha/2), n-1, lower.tail = FALSE)*s
Yi + c(-1, 1)*qt(1-(alpha/2), n-2, lower.tail = FALSE)*s
Yi + c(-1, 1)*qt(1-(alpha/2), n-3, lower.tail = FALSE)*s
Yi + c(-1, 1)*qt(1-(alpha/2), n-4, lower.tail = FALSE)*s
Yi + c(-1, 1)*qt(1-(alpha/2), n/2, lower.tail = FALSE)*s
Yi + c(-1, 1)*qt(1-(alpha/2), n - 6, lower.tail = FALSE)*s
Yi + c(-1, 1)*qt(1-(alpha/2), n - 12, lower.tail = FALSE)*s
Yi + c(-1, 1)*qt(1-(alpha/2), n - 11, lower.tail = FALSE)*s
Yi + c(-1, 1)*qt(1-(alpha/2), n - 10, lower.tail = FALSE)*s
fit <- lm(y ~ x)
predict(fit, data.frame(x = 3), interval = "prediction")
data(mtcars)
data <- mtcars
# Consider values to select variables as type factor
data$cyl <- factor(data$cyl)
data$vs <- factor(data$vs)
data$gear <- factor(data$gear)
data$carb <- factor(data$carb)
data$am <- factor(data$am,labels=c("Automatic","Manual"))
# Model Selection
simpleFit <- lm(mpg ~ am, data = data)
fit1 <- lm(mpg ~ am + wt, data = data)
fit2 <- lm(mpg ~ am + wt + qsec, data = data)
completeFit <- lm(mpg ~ ., data = data)
anova(simpleFit, fit1, fit2, completeFit)
finalFit <- fit2
# Summary of selected fit
summary(finalFit)
bestFit <- step(completeFit, direction = "both")
summary(bestFit)  # same result as finalFit
# Confidence Interval for Transmission Coefficient
amCoef <- coef(summary(finalFit))["am", "Estimate"]
stdE <- coef(summary(finalFit))["am", "Std. Error"]
t <- qt(0.975, n - 4)
amCoef + c(-1, 1)*t*stdE
summary(finalFit)
amCoef <- coef(summary(finalFit))["am", "Estimate"]
stdE <- coef(summary(finalFit))["am", "Std. Error"]
t <- qt(0.975, n - 4)
amCoef + c(-1, 1)*t*stdE
require(GGally); require(ggplot2)
fitVars <- data[, c("am", "wt", "qsec")]
ggpairs(fitVars, lower = list(continuous = "smooth"), params = c(method = "loess"))
data(mtcars)
mtdata <- mtcars
# Consider values to select variables as type factor
mtdata$cyl <- factor(mtdata$cyl)
mtdata$vs <- factor(mtdata$vs)
mtdata$gear <- factor(mtdata$gear)
mtdata$carb <- factor(mtdata$carb)
mtdata$am <- factor(mtdata$am,labels=c("Automatic","Manual"))
simpleFit <- lm(mpg ~ am, data = mtdata)
fit1 <- lm(mpg ~ am + wt, data = mtdata)
fit2 <- lm(mpg ~ am + wt + qsec, data = mtdata)
completeFit <- lm(mpg ~ ., data = mtdata)
anova(simpleFit, fit1, fit2, completeFit)
finalFit <- fit2
summary(finalFit)
amCoef <- coef(summary(finalFit))["am", "Estimate"]
coef(summary(finalFit))["am", "Estimate"]
coef(summary(finalFit))
data(mtcars)
mtdata <- mtcars
# Consider values to select variables as type factor
mtdata$cyl <- factor(mtdata$cyl)
mtdata$vs <- factor(mtdata$vs)
mtdata$gear <- factor(mtdata$gear)
mtdata$carb <- factor(mtdata$carb)
mtdata$am <- factor(mtdata$am,labels=c("Automatic","Manual"))
# Model Selection
simpleFit <- lm(mpg ~ am, data = mtdata)
fit1 <- lm(mpg ~ am + wt, data = mtdata)
fit2 <- lm(mpg ~ am + wt + qsec, data = mtdata)
completeFit <- lm(mpg ~ ., data = mtdata)
anova(simpleFit, fit1, fit2, completeFit)
finalFit <- fit2
# Summary of selected fit
summary(finalFit)
amCoef <- coef(summary(finalFit))["amManual", "Estimate"]
stdE <- coef(summary(finalFit))["amManual", "Std. Error"]
t <- qt(0.975, length(mtdata$mpg) - 4)
amCoef + c(-1, 1)*t*stdE
fitVars <- mtcars[, c("am", "wt", "qsec")]
ggpairs(fitVars, lower = list(continuous = "smooth"), params = c(method = "loess"))
fitVars <- mtcars[, c("mpg", "am", "wt", "qsec")]
ggpairs(fitVars, lower = list(continuous = "smooth"), params = c(method = "loess"))
boxplot(mtdata[mtdata$am == 0,]$mpg, mtdata[mtdata$am == 1, ]$mpg,
names = c("Automatic", "Manual"))
par(mar = c(2, 2, 2, 2))
boxplot(mtdata[mtdata$am == 0,]$mpg, mtdata[mtdata$am == 1, ]$mpg,
names = c("Automatic", "Manual"))
ggplot(mtdata, aes(am, mpg)) + geom_boxplot()
ggplot(mtdata, aes(y = mpg, x = am)) + geom_boxplot()
install.packages("car")
library(car)
leveneTest(mpg ~ am, data = mtdata)
plot(finalFit)
par(mrow = c(2, 2))
plot(finalFit)
leverage <- hatvalues(finalFit)
tail(sort(leverage), 5)
influence <- dfbetas(finalFit)
tail(sort(influence[, "amManual"]), 5)
data(mtcars)
mtdata <- mtcars
# Consider values to select variables as type factor
mtdata$cyl <- factor(mtdata$cyl)
mtdata$vs <- factor(mtdata$vs)
mtdata$gear <- factor(mtdata$gear)
mtdata$carb <- factor(mtdata$carb)
mtdata$am <- factor(mtdata$am,labels=c("Automatic","Manual"))
# Model Selection
simpleFit <- lm(mpg ~ am, data = mtdata)
fit1 <- lm(mpg ~ am + wt, data = mtdata)
fit2 <- lm(mpg ~ am + wt + qsec, data = mtdata)
completeFit <- lm(mpg ~ ., data = mtdata)
anova(simpleFit, fit1, fit2, completeFit)
finalFit <- fit2
# Summary of selected fit
summary(finalFit)
# Confidence Interval for Transmission Coefficient
amCoef <- coef(summary(finalFit))["amManual", "Estimate"]
stdE <- coef(summary(finalFit))["amManual", "Std. Error"]
t <- qt(0.975, length(mtdata$mpg) - 4)
amCoef + c(-1, 1)*t*stdE
# Leverage
leverage <- hatvalues(finalFit)
tail(sort(leverage), 5)
# Influence
influence <- dfbetas(finalFit)
tail(sort(influence[, "amManual"]), 5)
par(mfrow = c(2, 2))
plot(finalFit)
leverage <- hatvalues(finalFit)
tail(sort(leverage), 10)
# Influence
influence <- dfbetas(finalFit)
tail(sort(influence[, "amManual"]), 10)
data(mtcars)
cor(mtcars)
x <- 5
data(mtcars)
lm(mpg ~ factor(cyl) + wt, data = mtcars)
lm(mpg ~ factor(cyl), data = mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl)*wt, data = mtcars)
anova(fit1, fit2)
fit1
lm(mpg ~ I(wt*0.5) + factor(cyl), data = mtcars)
?mtcars
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
hatvalues(fit)
?influence.measures
influence.measures(fit)
library(swirl)
swirl()
plot(child ~ parent, galton)
plot(jitter(child, 4) ~ parent, galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd = 3, col = 'red')
summary(regrline)
fit <- lm(child ~ parent, galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
lhs - lhs
lhs - rhs
all.equal(lhs, rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild, varRes + varEst)
efit <- lm(accel ~ mag + dist, attenu)
mean(efit$residuals)
cov(efit$residuals, attenu$mag)
cov(efit$residuals, attenu$dist)
library(swirl)
swirl()
cor(gpa_nor, gch_nor)
l_nor <- lm(gch_nor ~gpa_nor)
q()
library(MASS)
head(shuttle)
shuttle2 <- shuttle
shuttle2$use2 <- as.numeric(shuttle$use == 'auto')
fit(glm(use2 ~ factor(wind) - 1, family = binomial, data = shuttle2))
fit <- glm(use2 ~ factor(wind) - 1, family = binomial, data = shuttle2)
summary(fit)
exp(coef(fit))
expcoef <- exp(coef(fit))
expcoef[1]/expcoef[2]
fit <- glm(use2 ~ factor(wind) + factor(magn) - 1, family = binomial, data = shuttle)
fit <- glm(use2 ~ factor(wind) + factor(magn) - 1, family = binomial, data = shuttle2)
exp(coef(fit))
1.4383682/1.4851533
library(swirl)
swirl()
fit <- lm(child ~parent, dataset = galton)
fit <- lm(child ~ parent, dataset = galton)
fit <- lm(child ~ parent, dataset = galton)
fit <- lm(child ~ parent, dataset = Galton)
fit <- lm(child ~ parent, data = galton)
sqrt((sum((fit$residuals)^2))/(n-2))
sqrt((sum(fit$residuals^2))/(n-2))
sqrt((sum((fit$residuals^2))/(n-2))
dnbinom(    )
sqrt(sum((fit$residuals)^2)/(n-2))
sqrt(sum(fit$residuals^2)/(n-2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu <- mean(galton$child)
sTOt <- sum((galton$child - mu)^2)
sTOt <- sum((galton$child-mu)^2)
sTot <- sum((galton$child-mu)^2)
sRes <- sum(fit$residuals)
sRes <- deviance(fit)
1 - sRes/sTot
summary(fit)$r.squared
cor(galton$child, galton$parent)^2
ones <- rep(1, nrow(galton))
lm(child ~ ones + parent - 1, galton)
lm(child ~ parent - 1)
lm(child ~ parent - 1, galton)
lm(child ~ parent, galton)
lm(child ~1)
lm(child ~ 1, galton)
head(tress)
head(trees)
fit <- lm(Volume ~ Girth + Height + Constant - 1, trees)
trees2 <- eliminate("Girth", trees)
head(trees)
head(trees2)
fit2 <- lm(Volume ~ Height + Constant - 1, trees2)
lapply(list(fit, fit2), coef)
q()
q()
install.packages(c("AppliedPredictiveModeling", "caret"))
library(AppliedPredictiveModeling); library(caret); data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
head(adData)
testIndex <- createDataPartition(diagnosis, p = 0.50, list = FALSE)
head(testIndex)
training <- adData[-testIndex,]
testing <- adData[testIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training)
hist(training$Superplasticizer)
hist(log10(training$Superplasticizer)
)
hist(log10(training$Superplasticizer+1))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_str <- grep("^ÃŽL", colnames(training), value = TRUE)
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
preProc$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_str <- grep("^IL", colnames(training), value = TRUE)
trainingIL <- training[, IL_str]
rm(trainingIL)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_str <- grep("IL", colnames(training), value = TRUE)
predictorsIL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictorsIL)
inTrain <- createDataPartition(df$diagnosis, p = 3/4)[[1]]
training <- df[inTrain,]
testing <- df[-inTrain,]
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
install.packages("e1071")
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, testing)
confusionMatrix(predictions, testing$diagnosis)
modelFit <- train(diagnosis ~ ., method = "glm", preProcess = "pca", data = training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
confusionMatrix(testing$diagnosis, predict(modelFit, testing))
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, testing)
confusionMatrix(testing$diagnosis, predictions)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL-str <- grep("IL", colnames(training), values = TRUE)
IL-str <- grep("IL", colnames(training), value = TRUE)
IL-str <- grep("^IL", colnames(training), value = TRUE)
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.9)
preProc$rotation
setwd("~/Coursera/dataScienceCoursera/PracticalMachineLearning/Project")
library(caret)
library(rpart)
library(RColorBrewer)
library(rattle)
library(randomForest)
# Load Data
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings = c("NA", "", "#DIV/0!"))
testing <- read.csv(url(testUrl), na.strings = c("NA", "", "#DIV/0!"))
install.packages("rattle")
install.packages("randomForest")
summary(training)
nearZero <- nearZeroVar(training, saveMetrics = TRUE)
1:7
c(1:7)
-1:7
-(1:7)
training <- training[, nearZero$nzv == FALSE]
training <- training[, -(1:7)]
training <- read.csv(url(trainUrl), na.strings = c("NA", "", "#DIV/0!"))
set.seed(24)
inTrain <- createDataPartition(y = training$classe, p = 2/3, list = FALSE)
train <- training[inTrain, ]
test <- training[-inTrain, ]
nearZero <- nearZeroVar(train, saveMetrics = TRUE)
train <- train[, nearZero$nzv == FALSE]
train <- train[, -(1:7)]
sapply(training, function(x) {sum(is.na(x))})
sapply(train, function(x) {sum(is.na(x))})
testNA <- sapply(train, function(x) {sum(is.na(x))})
table(testNA)
countNA <- sapply(training, function(x) {sum(is.na(x))})
table(countNA)
naColumns <- names(countNA[countNA != 0])
train <- train[, -naColumns]
train <- train[, !names(train) %in% naColumns]
str(train)
train <- train[, -c(naColumns)]
preProc1 <- colnames(train)
preProc2 <- colnames(train[, -c("classe")])
preProc2 <- colnames(train[, -52]) # remove classe var
test <- test[preProc1]
testing <- testing[preProc2]  # we only need predictors to get the answers
control <- trainControl(method = "repeatedcv", number = 4, repeats = 2)
gbmFit <- train(classe ~ ., data = train, method = "gbm", trControl = control,
verbose = FALSE)
control <- trainControl(method = "repeatedcv", number = 4, repeats = 2)
gbmFit <- train(classe ~ ., data = train, method = "gbm", trControl = control,
verbose = FALSE)
# Random Forests
rfFit <- randomForest(classe ~ ., data = train)
library(caret)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(caret)
library(RColorBrewer)
library(rattle)
library(randomForest)
control <- trainControl(method = "repeatedcv", number = 4, repeats = 2)
gbmFit <- train(classe ~ ., data = train, method = "gbm", trControl = control,
verbose = FALSE)
library(gbm)
control <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
gbmFit <- train(classe ~ ., data = train, method = "gbm", trControl = control,
verbose = FALSE)
# Random Forests
rfFit <- randomForest(classe ~ ., data = train)
gbmPrediction <- predict(gbmFit, newdata = test)
gbmCM <- confusionMatrix(gbmPrediction, test$classe)
rfPrediction <- predict(rfFit, newdata = test, type = "class")
rfCM <- confusionMatrix(rfPrediction, test$classe)
gbmCM
rfCM
testingPrediction <- predict(rfFit, newdata = testing, type = "class")
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(testingPrediction)
nearZero <- nearZeroVar(train, saveMetrics = TRUE)
train <- train[, nearZero$nzv == FALSE]
train <- train[, -(1:6)]
countNA <- sapply(training, function(x) {sum(is.na(x))})
table(countNA)
naColumns <- names(countNA[countNA != 0])
train <- train[, !names(train) %in% naColumns]
set.seed(24)
# Partitioning training set
inTrain <- createDataPartition(y = training$classe, p = 2/3, list = FALSE)
train <- training[inTrain, ]
test <- training[-inTrain, ]
# Preprocessing
nearZero <- nearZeroVar(train, saveMetrics = TRUE)
train <- train[, nearZero$nzv == FALSE]
train <- train[, -(1:6)]
countNA <- sapply(training, function(x) {sum(is.na(x))})
table(countNA)
naColumns <- names(countNA[countNA != 0])
train <- train[, !names(train) %in% naColumns]
preProc1 <- colnames(train)
preProc2 <- colnames(train[, -53]) # remove classe var
test <- test[preProc1]
testing <- testing[preProc2]
rfCM$overall
rfCM$overall[1]
cbind(gbmCM$overall[1], rfCM$overall[1])
